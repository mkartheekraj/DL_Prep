{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BGA1391_mTcd"
      },
      "source": [
        "# Prepare and Analyze Time Series - Milestone 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UCoKyonxmTcq"
      },
      "source": [
        "This Jupyter notebook serves as a partial solution to Milestone 3 of the liveProject on End-to-end Time Series Forecasting with Deep Learning.\n",
        "\n",
        "You can upload this notebook to Colab and work from there. Alternatively, you can also work on this notebook in your local environment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X-4J5BdjmTdE"
      },
      "source": [
        "Great job on completing previous Milestones and reaching the final MileStone! We have processed, cleaned and analyzed the data so we can now proceed with our modeling as shown in the diagram below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iUWpopHgmTdW"
      },
      "source": [
        "![Milestone 3](https://s3.ap-southeast-1.amazonaws.com/www.jiahao.io/manning/project1_milestone3.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vk6K1WLTmTdo"
      },
      "source": [
        "In this final Milestone 3, we shall complete 3 tasks:\n",
        "1. Build baseline model using Naive and sNaive methods\n",
        "2. Understand the various metrics for evaluation of time series model and decide on our metrics\n",
        "3. Set up model using Facebook Prophet for comparison\n",
        "\n",
        "Without further ado, let's begin!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NDvJNgLbmTdw"
      },
      "source": [
        "## Importing Necessary Libraries and Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NCTwtClomTeG"
      },
      "source": [
        "Let us first import the necessary libraries and load the data that we will be working with throughout this Milestone. \n",
        "\n",
        "The data (data/sales_cleaned.csv) that we are using is an output from previous Milestone. \n",
        "\n",
        "Again, recall that in this liveProject, you are a data scientist at a large retailer and your challenge is to forecast the sales of the respective stores by each category for the next 28 days."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JEG0WY0pmTeh",
        "outputId": "5d0a93b0-dd28-4160-f93e-0b6e56384b76"
      },
      "outputs": [],
      "source": [
        "RunningInCOLAB = 'google.colab' in str(get_ipython())\n",
        "\n",
        "# import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm import tqdm\n",
        "if RunningInCOLAB:\n",
        "  !pip install prophet\n",
        "from prophet import Prophet\n",
        "import itertools\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "# suppress pandas SettingWithCopyWarning \n",
        "pd.options.mode.chained_assignment = None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8WIRkwOnmTe7"
      },
      "source": [
        "## Building our Baseline Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I3UE3pZnmTfk"
      },
      "source": [
        "In order to know how well our models are performing, we need a baseline model for reference. The baseline model will use a very quick and simple method (in our case, Naive or sNaive method) and our models must have better performance than the baseline to be useful."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z1upSjbXmTfr"
      },
      "source": [
        "The Naive method uses the last known observation of the time series as forecasts for the next 28 days.\n",
        "\n",
        "The Seasonal Naive (sNaive) method is similar to the Naive method, but this time the forecasts of the model are equal to the last known observation of the same seasonal period. In our case, we will set the period as 7 days based on our analysis in previous Milestone."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AQA8bLjmmTfs"
      },
      "source": [
        "<ins>Instructions</ins>:<br>\n",
        "- Let's read in our sales_cleaned.csv data and paste our train-test split function from previous Milestone."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<ins>Hints</ins> (click when needed):<br>\n",
        "- [Follow the example code here to upload files to Colab from your local file system](https://colab.research.google.com/notebooks/io.ipynb)\n",
        "- [Use Pandas `read_csv` function to load CSV data](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89,
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "ok": true,
              "status": 200,
              "status_text": ""
            }
          }
        },
        "id": "arEZHWSam7gA",
        "outputId": "db1bb9f7-ffbf-440d-8863-d7d80be27e52"
      },
      "outputs": [],
      "source": [
        "# upload file from Colab\n",
        "if RunningInCOLAB:\n",
        "  from google.colab import files\n",
        "\n",
        "  uploaded = files.upload()\n",
        "\n",
        "  for fn in uploaded.keys():\n",
        "    print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "        name=fn, length=len(uploaded[fn])))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "TS8isDvwmTf4",
        "outputId": "5d601fc6-b701-41a7-af83-901159873e4b"
      },
      "outputs": [],
      "source": [
        "# read in our processed data\n",
        "# by using parse_dates in parameter of read_csv, we can convert date column to datetime format without additional step\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QZA67XNymTgD"
      },
      "outputs": [],
      "source": [
        "# copy and paste the train-test split function from previous Milestone\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c_wn8e3pmTgI"
      },
      "source": [
        "<ins>Instructions</ins>:<br>\n",
        "- Now let's start off with implementing the Naive method. Develop a function that will take in the training and test data and return a dataframe containing the test data and corresponding predictions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<ins>Hints</ins> (click when needed):<br>\n",
        "- [Refer to the section on \"Start with a Naive Approach\" to understand more about the Naive method](https://www.analyticsvidhya.com/blog/2018/02/time-series-forecasting-methods/)\n",
        "\n",
        "<ins>Examples</ins>:<br>\n",
        "```\n",
        ">>> def naive_predictions(training_df, test_df):\n",
        "...     naive_pred_df = training_df.loc[training_df.date == training_df.date.max(), ['id','x']]\n",
        "...     naive_pred_df.rename(columns={'x':'naive_pred'}, inplace=True)\n",
        "...     naive_test_df = test_df.merge(naive_pred_df, on='id', how='left')\n",
        "...     return naive_test_df\n",
        "\n",
        ">>> training_df = pd.DataFrame({'date': ['2015-02-15', '2015-02-16', '2015-02-17'], 'id': ['a', 'a', 'a'], 'x': [3, 7, 2]})\n",
        ">>> test_df = pd.DataFrame({'date': ['2015-02-18'], 'id': ['a'], 'x': [9]})\n",
        "\n",
        ">>> naive_predictions(training_df, test_df)\n",
        "         date id  x  naive_pred\n",
        "0  2015-02-18  a  9           2\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GXRTWH-_mTgK"
      },
      "outputs": [],
      "source": [
        "# Implement the Naive method function\n",
        "def naive_predictions(training_df, test_df):\n",
        "    \"\"\"\n",
        "    Implement the Naive method and return dataframe with test data and corresponding predictions\n",
        "    \"\"\"\n",
        "    \n",
        "    return "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "3H65LyQImTgN",
        "outputId": "292b5fa4-647e-4fff-b4fb-c34df2cd798b"
      },
      "outputs": [],
      "source": [
        "# Naive method for first cv split\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aIqPpMldmTgP"
      },
      "source": [
        "Let's plot our Naive predictions against real data for visualization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N1bLUEUxmTgQ"
      },
      "source": [
        "<ins>Instructions</ins>:<br>\n",
        "- For a given time series, plot the Naive predictions against actual sales"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<ins>Hints</ins> (click when needed):<br>\n",
        "- [Use Seaborn `lineplot` to plot line plots](https://seaborn.pydata.org/generated/seaborn.lineplot.html)\n",
        "\n",
        "<ins>Examples</ins>:<br>\n",
        "```\n",
        ">>> df = pd.DataFrame({'date': ['2015-02-15', '2015-02-16', '2015-02-17'], 'y1': [4, 6, 5], 'y2': [3, 7, 2]})\n",
        "\n",
        ">>> sns.lineplot(x=df.date, y=df.y1)\n",
        ">>> sns.lineplot(x=df.date, y=df.y2)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 334
        },
        "id": "iM0imdOMmTgS",
        "outputId": "3d6e86a7-ad90-4c13-bdf5-a5ba3324dd7c"
      },
      "outputs": [],
      "source": [
        "# function for plotting predictions against actual sales given series_id\n",
        "def plot_pred(prediction_test_df, series_id, yhat):\n",
        "    \"\"\"\n",
        "    Plot the predictions against actual sales for a specified time series\n",
        "    \"\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot the predictions against actual sales for a specified time series\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AVgK1EurmTgU"
      },
      "source": [
        "From our plot, we can see that the Naive predictions are uninteresting; just a horizontal straight line of constant value. Well, it's naive after all.\n",
        "\n",
        "Now, let's implement the sNaive method. Remember, the seasonal period for our case is 7 days."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4uutxx7EmTgZ"
      },
      "source": [
        "<ins>Instructions</ins>:<br>\n",
        "- Implement the sNaive method using the last 7 known observations of each time series. Develop a function that will take in the training and test data and return a dataframe containing the test data and corresponding predictions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<ins>Hints</ins> (click when needed):<br>\n",
        "- [sNaive notes (read on what to do when prediction_length > season_length)](https://ts.gluon.ai/api/gluonts/gluonts.model.seasonal_naive.html)\n",
        "> \"\"\"\n",
        "\n",
        "> If prediction_length > season_length, then the season is repeated multiple times\n",
        "\n",
        "> \"\"\"\n",
        "\n",
        "<ins>Examples</ins>:<br>\n",
        "```\n",
        ">>> def snaive_predictions(training_df, test_df):\n",
        "...     training_df['dayofweek'] = training_df['date'].dt.weekday\n",
        "...     training_df.sort_values(by='date', ascending=False, inplace=True)\n",
        "...     snaive_pred_df = training_df[:7][['dayofweek', 'x']]\n",
        "...     snaive_pred_df.rename(columns={'x':'snaive_pred'}, inplace=True)\n",
        "...     test_df['dayofweek'] = test_df['date'].dt.weekday\n",
        "...     snaive_test_df = test_df.merge(snaive_pred_df, on=['dayofweek'], how='left')\n",
        "...     return snaive_test_df\n",
        "\n",
        ">>> # dummy data\n",
        ">>> training_df = pd.DataFrame({'date': ['2015-02-15', '2015-02-16', '2015-02-17'], 'id': ['a', 'a', 'a'], 'x': [3, 7, 2]})\n",
        ">>> training_df['date'] = pd.to_datetime(training_df['date'])\n",
        ">>> test_df = pd.DataFrame({'date': ['2015-02-22'], 'id': ['a'], 'x': [9]})\n",
        ">>> test_df['date'] = pd.to_datetime(test_df['date'])\n",
        "\n",
        ">>> snaive_predictions(training_df, test_df)\n",
        "        date id  x  dayofweek  snaive_pred\n",
        "0 2015-02-22  a  9          6            3\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4cP3ZQ7_mTgd"
      },
      "outputs": [],
      "source": [
        "# Implement the sNaive method in a function\n",
        "def snaive_predictions(training_df, test_df):\n",
        "    \"\"\"\n",
        "    Implement the sNaive method and return dataframe with test data and corresponding predictions\n",
        "    \"\"\"\n",
        "    \n",
        "    \n",
        "    return "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 597
        },
        "id": "g0MGVdl1mTgg",
        "outputId": "646ffd2b-4c41-4ccf-8d11-43a1795f8ede"
      },
      "outputs": [],
      "source": [
        "# sNaive predictions for first cv split\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lG0RBihKmTgg"
      },
      "source": [
        "Similarly, let's plot our sNaive predictions against the actual sales for a random time series."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Zd-z4PNmTgg"
      },
      "source": [
        "<ins>Instructions</ins>:<br>\n",
        "- Plot the sNaive predictions against the actual sales for a time series"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<ins>Hints</ins> (click when needed):<br>\n",
        "- [Use Seaborn `lineplot` to plot line plots](https://seaborn.pydata.org/generated/seaborn.lineplot.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 334
        },
        "id": "yISHVcjqmTgi",
        "outputId": "e77a59e7-b420-4c5d-9eca-b9365f99eccc"
      },
      "outputs": [],
      "source": [
        "# plot sNaive predictions against actual sales for a time series\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q9djOlvhmTgn"
      },
      "source": [
        "This time round, the sNaive predictions seem much more realistic and is somehow able to follow the oscillations of the actual sales."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tnpPwS1RmTgo"
      },
      "source": [
        "Intuitively, we know that the sNaive method will be a better baseline than the Naive method. But rather than seeing random plots comparing the predictions against actual sales, is there a score that we can use to summarise the model performance?\n",
        "\n",
        "This is what we shall set out to do in the next section below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f5Cf6PsTmTgq"
      },
      "source": [
        "## Choosing our Metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r_Yf7_VJmTgq"
      },
      "source": [
        "In time series, there are various metrics that we can use to evaluate our models.\n",
        "\n",
        "Based on Prof Rob Hyndman in this [paper](https://robjhyndman.com/papers/foresight.pdf), there are four types of metrics for time series forecasting. In our case, we shall compute Mean Absolute Error (MAE) and Mean Absolute Percentage Error (MAPE) for each time series.\n",
        "\n",
        "In addition, because we have multiple time series, we need to somehow weigh the score for each time series and combine them into a single overall score. In our case, we shall weigh the time series based on the cumulative actual dollar sales, which is computed using the last 28 observations of the training data. This weightage method follows the [M5 competition in Kaggle](https://www.kaggle.com/c/m5-forecasting-accuracy/overview/evaluation). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h3B-KMwJmTgq"
      },
      "source": [
        "<ins>Instructions</ins>:<br>\n",
        "- Implement a function that given a time series, computes the MAE\n",
        "- Implement a function that given a training and test data, computes the weighted MAE\n",
        "- Implement code to calculate the cross-validated weighted MAE for the Naive and sNaive models respectively. Use 3 splits."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<ins>Hints</ins> (click when needed):<br>\n",
        "- [Refer to this link for the MAE formula](https://en.wikipedia.org/wiki/Mean_absolute_error)\n",
        "- [Read page 8 of this document to understand how weighting is performed](https://mofc.unic.ac.cy/wp-content/uploads/2020/03/M5-Competitors-Guide-Final-10-March-2020.docx)\n",
        "\n",
        "<ins>Examples</ins>:<br>\n",
        "- To calculate MAE\n",
        "    ```\n",
        "    >>> df = pd.DataFrame({'y_hat': [4, 6, 2], 'y': [3, 7, 2]})\n",
        "    \n",
        "    >>> df['abs_error'] = (df['y_hat'] - df['y']).abs()\n",
        "    >>> df['abs_error'].mean()\n",
        "    0.6666666666666666\n",
        "    \n",
        "    ```\n",
        "\n",
        "- To calculate weighted MAE given a list of MAEs and sales\n",
        "    ```\n",
        "    >>> sales_list = [100, 200, 150]\n",
        "    >>> mae_list = [20, 30, 15]\n",
        "\n",
        "    >>> overall_sales = np.sum(sales_list)\n",
        "    >>> weights_list = [s/overall_sales for s in sales_list]\n",
        "    >>> wmae_list = [a*b for a,b in zip(mae_list, weights_list)]  # multiply weight to MAE\n",
        "    >>> np.sum(wmae_list)  # Weighted MAE\n",
        "    22.77777777777778\n",
        "    ```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7EGSX_RZmTgu"
      },
      "outputs": [],
      "source": [
        "# implement functions to compute MAE and weighted MAE\n",
        "def compute_mae(training_df, prediction_test_df, y, y_hat, series_id):\n",
        "    \"\"\"\n",
        "    Given a time series ID, compute the MAE for that time series, and return the \n",
        "    computed MAE and the last 28-day training sales\n",
        "    \"\"\"\n",
        "    \n",
        "    return \n",
        "\n",
        "\n",
        "def compute_wmae(training_df, prediction_test_df, y, y_hat):\n",
        "    \"\"\"\n",
        "    Given a training and prediction data, compute and return the weighted MAE\n",
        "    \"\"\"\n",
        "    \n",
        "    return \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6yqFXJYzmTgz",
        "outputId": "44202002-4450-48f5-9abf-ab609172f2ed"
      },
      "outputs": [],
      "source": [
        "# compute the cross-validated weighted MAE for the Naive model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "igSvNMLRmTgz",
        "outputId": "b0cf7447-e6db-4b78-fed9-adfa7bf1289e"
      },
      "outputs": [],
      "source": [
        "# compute the cross-validated weighted MAE for the sNaive model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M9B4rgcEmTg0"
      },
      "source": [
        "Based on the 3-split cross-validated weighted MAE, the sNaive method has almost half the weighted MAE of the Naive method."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B6kvCbJImTg1"
      },
      "source": [
        "<ins>Instructions</ins>:<br>\n",
        "Similar to the computation for MAE, we shall now implement the same for MAPE\n",
        "- Implement a function that given a time series, computes the MAPE\n",
        "- Implement a function that given a training and test data, computes the weighted MAPE\n",
        "- Implement code to calculate the cross-validated weighted MAPE for the Naive and sNaive models respectively. Use 3 splits."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<ins>Hints</ins> (click when needed):<br>\n",
        "- [Refer to this link for the MAPE formula](https://en.wikipedia.org/wiki/Mean_absolute_percentage_error)\n",
        "- [Read page 8 of this document to understand how weighting is performed](https://mofc.unic.ac.cy/wp-content/uploads/2020/03/M5-Competitors-Guide-Final-10-March-2020.docx)\n",
        "\n",
        "<ins>Examples</ins>:<br>\n",
        "- To calculate MAPE\n",
        "    ```\n",
        "    >>> df = pd.DataFrame({'y_hat': [4, 6, 2], 'y': [3, 7, 2]})\n",
        "\n",
        "    >>> df['abs_pct_error'] = ((df['y'] - df['y_hat'])/df['y']).abs()\n",
        "    >>> df['abs_pct_error'].mean()\n",
        "    0.15873015873015872\n",
        "\n",
        "    ```\n",
        "\n",
        "- To calculate weighted MAPE given a list of MAPEs and sales\n",
        "    ```\n",
        "    >>> sales_list = [100, 200, 150]\n",
        "    >>> mape_list = [0.2, 0.3, 0.15]\n",
        "\n",
        "    >>> overall_sales = np.sum(sales_list)\n",
        "    >>> weights_list = [s/overall_sales for s in sales_list]\n",
        "    >>> wmape_list = [a*b for a,b in zip(mape_list, weights_list)]  # multiply weight to MAPE\n",
        "    >>> np.sum(wmape_list)  # Weighted MAPE\n",
        "    0.22777777777777777\n",
        "    ```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T_it-oZxmTg1"
      },
      "outputs": [],
      "source": [
        "# implement functions to compute MAPE and weighted MAPE\n",
        "def compute_mape(training_df, prediction_test_df, y, y_hat, series_id):\n",
        "    \"\"\"\n",
        "    Given a time series ID, compute the MAPE for that time series, and return the \n",
        "    computed MAPE and the last 28-day training sales\n",
        "    \"\"\"\n",
        "    \n",
        "    return \n",
        "\n",
        "\n",
        "def compute_wmape(training_df, prediction_test_df, y, y_hat):\n",
        "    \"\"\"\n",
        "    Given a training and prediction data, compute and return the weighted MAPE\n",
        "    \"\"\"\n",
        "    \n",
        "    return "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rPsmOzxImTg5",
        "outputId": "ef1fb7ef-a058-46e6-e869-70796c1212f5"
      },
      "outputs": [],
      "source": [
        "# compute the cross-validated weighted MAPE for the Naive model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BEuLwmUPmTg8",
        "outputId": "b83111ff-06fa-4ed4-d124-6440e9e5fc5f"
      },
      "outputs": [],
      "source": [
        "# compute the cross-validated weighted MAPE for the sNaive model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rtly4zPEmThM"
      },
      "source": [
        "Similar to the conclusions from analysing the MAE, the sNaive model has a lower weighted MAPE than the Naive model. \n",
        "\n",
        "We will therefore use the sNaive method as our baseline."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vJgVNjiumThW"
      },
      "source": [
        "## Forecasting with Facebook Prophet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zTJyUyBxmThW"
      },
      "source": [
        "Now that we have a baseline reference, let's implement a more sophisticated model for our forecasting.\n",
        "\n",
        "[Facebook Prophet](https://facebook.github.io/prophet/) is an open source software released by Facebook and is used in many applications across Facebook for producing reliable forecasts.\n",
        "\n",
        "We shall use Prophet to implement our model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As an example, suppose we have the following dummy data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Data:\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>date</th>\n",
              "      <th>sales</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2015-02-15</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2015-02-16</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2015-02-17</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        date  sales\n",
              "0 2015-02-15      3\n",
              "1 2015-02-16      7\n",
              "2 2015-02-17      2"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Data:\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>date</th>\n",
              "      <th>sales</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2015-02-18</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        date  sales\n",
              "0 2015-02-18      9"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# dummy data\n",
        "training_df = pd.DataFrame({'date': ['2015-02-15', '2015-02-16', '2015-02-17'], 'sales': [3, 7, 2]})\n",
        "training_df['date'] = pd.to_datetime(training_df['date'])\n",
        "test_df = pd.DataFrame({'date': ['2015-02-18'], 'sales': [9]})\n",
        "test_df['date'] = pd.to_datetime(test_df['date'])\n",
        "\n",
        "print(\"Training Data:\")\n",
        "display(training_df)\n",
        "print(\"Test Data:\")\n",
        "display(test_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can perform model training by this:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:prophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this.\n",
            "INFO:prophet:Disabling weekly seasonality. Run prophet with weekly_seasonality=True to override this.\n",
            "INFO:prophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n",
            "INFO:prophet:n_changepoints greater than number of observations. Using 1.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<prophet.forecaster.Prophet at 0x2289a4e2508>"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# renaming column names to suit Prophet\n",
        "training_df.rename(columns={'sales': 'y', 'date':'ds'}, inplace=True)\n",
        "m = Prophet()\n",
        "m.fit(training_df)  # training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To make our predictions, we can do this:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>date</th>\n",
              "      <th>sales</th>\n",
              "      <th>prophet_pred</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2015-02-18</td>\n",
              "      <td>9</td>\n",
              "      <td>3.019115</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        date  sales  prophet_pred\n",
              "0 2015-02-18      9      3.019115"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# make predictions\n",
        "future = m.make_future_dataframe(periods=1, include_history=False)\n",
        "prophet_pred_df = m.predict(future)[['ds', 'yhat']]\n",
        "prophet_pred_df.rename(columns={'ds':'date', 'yhat':'prophet_pred'}, inplace=True)\n",
        "prophet_test_df = test_df.merge(prophet_pred_df, on=['date'], how='left')\n",
        "prophet_test_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X4A438UfmThW"
      },
      "source": [
        "<ins>Instructions</ins>:<br>\n",
        "- Implement a model using Prophet to forecast the next 28 days sales for each time series\n",
        "- Test your model using the first CV split\n",
        "- Plot the Prophet predictions against the actual sales\n",
        "\n",
        "<ins>Bonus</ins>:<br>\n",
        "- Add in a [monthly seasonality](https://facebook.github.io/prophet/docs/seasonality,_holiday_effects,_and_regressors.html#specifying-custom-seasonalities) to your Prophet model to improve its accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<ins>Hints</ins> (click when needed):<br>\n",
        "- [Refer to this quickstart guide for further guidance on how to model using Prophet](https://facebook.github.io/prophet/docs/quick_start.html)\n",
        "\n",
        "<ins>Examples</ins>:<br>\n",
        "```\n",
        ">>> # dummy data\n",
        ">>> training_df = pd.DataFrame({'date': ['2015-02-15', '2015-02-16', '2015-02-17'], 'sales': [3, 7, 2]})\n",
        ">>> training_df['date'] = pd.to_datetime(training_df['date'])\n",
        ">>> test_df = pd.DataFrame({'date': ['2015-02-18'], 'sales': [9]})\n",
        ">>> test_df['date'] = pd.to_datetime(test_df['date'])\n",
        "\n",
        ">>> # renaming column names to suit Prophet\n",
        ">>> training_df.rename(columns={'sales': 'y', 'date':'ds'}, inplace=True)\n",
        ">>> m = Prophet()\n",
        ">>> m.fit(training_df)  # training\n",
        "\n",
        ">>> # make predictions\n",
        ">>> future = m.make_future_dataframe(periods=1, include_history=False)\n",
        ">>> prophet_pred_df = m.predict(future)[['ds', 'yhat']]\n",
        ">>> prophet_pred_df.rename(columns={'ds':'date', 'yhat':'prophet_pred'}, inplace=True)\n",
        ">>> prophet_test_df = test_df.merge(prophet_pred_df, on=['date'], how='left')\n",
        ">>> prophet_test_df\n",
        "        date  sales  prophet_pred\n",
        "0 2015-02-18      9      3.019115\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0u0BPz6tmThW"
      },
      "outputs": [],
      "source": [
        "# Function to train and predict sales using Prophet\n",
        "def prophet_predictions(training_df, test_df, cv, changepoint_prior_scale=0.05, changepoint_range=0.8):\n",
        "    \"\"\"\n",
        "    Train and predict sales using Prophet\n",
        "    \"\"\"\n",
        "    \n",
        "\n",
        "    return "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "id": "I4RNwtD5mThW",
        "outputId": "2f3f00d4-5386-4714-8de5-d2ec0cb93280"
      },
      "outputs": [],
      "source": [
        "# Run Prophet model for first CV split\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 334
        },
        "id": "4LFZ16iVmThW",
        "outputId": "faaee5ca-b806-45ce-8a2c-33fee26c3b9f"
      },
      "outputs": [],
      "source": [
        "# plot Prophet predictions against actual sales\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K4go6KgDmThZ"
      },
      "source": [
        "There are some key [hyperparameters](https://facebook.github.io/prophet/docs/trend_changepoints.html#automatic-changepoint-detection-in-prophet) of the Prophet model that we can tune. In our case, we shall experiment with 2 key hyperparameters:\n",
        "1. changepoint_range\n",
        "2. changepoint_prior_scale\n",
        "\n",
        "Recall the nested walk-forward validation strategy where we will use the validation set to determine our optimal hyperparameters first before fitting the model on both the training and validation set to evaluate on test data.\n",
        "\n",
        "![Walk Forward Validation](https://s3.ap-southeast-1.amazonaws.com/www.jiahao.io/manning/walk_forward_validation.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2d9Z-NwymThZ"
      },
      "source": [
        "<ins>Instructions</ins>:<br>\n",
        "- Setup a function to implement grid search that will return the optimal hyperparameters and the corresponding weighted MAE score. For our hyperparameters, we shall use \n",
        "    - 'changepoint_range': [0.8, 0.9]\n",
        "    - 'changepoint_prior_scale': [0.05, 0.1, 0.3]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<ins>Hints</ins> (click when needed):<br>\n",
        "- Use example of [hyperparameter tuning](https://facebook.github.io/prophet/docs/diagnostics.html#hyperparameter-tuning) from Prophet's documentation as reference\n",
        "- [Refer to the section on \"Adjusting trend flexibility\" to understand how `changepoint_prior_scale` affects the model](https://facebook.github.io/prophet/docs/trend_changepoints.html#adjusting-trend-flexibility)\n",
        "\n",
        "<ins>Examples</ins>:<br>\n",
        "- Grid search example\n",
        "    ```\n",
        "    >>> param_grid = {'changepoint_range': [0.8, 0.9]}  \n",
        "\n",
        "    >>> all_params = [dict(zip(param_grid.keys(), v)) for v in itertools.product(*param_grid.values())]\n",
        "\n",
        "    >>> best_result = np.inf\n",
        "    >>> best_params = None\n",
        "\n",
        "    >>> for param_set in all_params:\n",
        "    ...     wmae_list = []\n",
        "    ...     for i in range(cv):\n",
        "    ...         training_df, validation_df, _ = get_cv_split(sales_df, i, validation=True)\n",
        "    ...         prophet_eval_df = prophet_predictions(training_df, validation_df, i, **param_set)\n",
        "    ...         wmae = compute_wmae(training_df, prophet_eval_df, 'sales', 'prophet_pred')\n",
        "    ...         wmae_list.append(wmae)\n",
        "    ...     overall_wmae = np.mean(wmae_list)\n",
        "    ...     if overall_wmae < best_result:\n",
        "    ...         best_params = param_set\n",
        "    ...         best_result = overall_wmae\n",
        "            \n",
        "    ```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FwcL2nk1mThZ"
      },
      "outputs": [],
      "source": [
        "# Function to implement grid search with the Prophet model and return optimal hyperparameters\n",
        "def grid_search_prophet(cv=3):\n",
        "    \"\"\"\n",
        "    Implement grid search with the Prophet model and return optimal hyperparameters and weighted MAE\n",
        "    \"\"\"\n",
        "    \n",
        "    \n",
        "    return "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Execute function to run grid search\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E_rk_O2bmThc"
      },
      "source": [
        "Great, now we have our optimal hyperparameters. For our last step, we shall evaluate our Prophet model with the optimal hyperparameters. Needless to say, our Prophet model should be better than the sNaive model.\n",
        "\n",
        "Ps: After all the evaluations are done, remember to set predictions on Christmas to 0."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qcB9ckuymThe"
      },
      "source": [
        "<ins>Instructions</ins>:<br>\n",
        "- Evaluate Prophet model with optimal hyperparameters by computing the 3-split cross-validated weighted MAE and weighted MAPE on the test data\n",
        "- Compare your results against the sNaive baseline results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1iU-2pjYmThf",
        "outputId": "1f78eab8-0705-4efe-e5d4-877c9cab1ca6"
      },
      "outputs": [],
      "source": [
        "# compute weighted MAE for Prophet predictions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PuSLYK4jmThf",
        "outputId": "313ca618-156f-4d51-af5d-ce4e14b1d20a"
      },
      "outputs": [],
      "source": [
        "# compute weighted MAPE for Prophet predictions\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nEdKyj7vmThg"
      },
      "source": [
        "## Conclusion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ziOpDCOmThg"
      },
      "source": [
        "Great job on completing this last Milestone. With that, you have completed the liveProject and learnt to:\n",
        "- Process and clean a time series data\n",
        "- Analyse time series and determine seasonality\n",
        "- Train-test split time series data using nested walk-forward validation\n",
        "- Establish baseline models and optimize Prophet model\n",
        "- Evaluate various models using weighted MAE and MAPE\n",
        "\n",
        "That's quite an accomplishment so give yourself a pat on the back.\n",
        "\n",
        "In the next liveProject of this series, we shall investigate the use of deep learning models for forecasting. Hope to see you there :)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Prepare and Analyze Time Series - Milestone 3 Solution.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "b7bc2e76cbe5fb8595f4ba211d5aadd6fa3b8e13e3681bdb0f26937058245569"
    },
    "kernelspec": {
      "display_name": "Python 3.7.10 64-bit ('manning': conda)",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.10"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
